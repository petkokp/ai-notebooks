{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Neural Turing Machines\" paper implementation - https://arxiv.org/pdf/1410.5401.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from model.model import EncapsulatedNTM\n",
    "\n",
    "class CopyTaskParams:\n",
    "    def __init__(self, name=\"copy-task\", controller_size=100, controller_layers=1, num_heads=1,\n",
    "                 sequence_width=8, sequence_min_len=1, sequence_max_len=20, memory_n=128, memory_m=20,\n",
    "                 num_batches=5000, batch_size=1, rmsprop_lr=1e-4, rmsprop_momentum=0.9, rmsprop_alpha=0.95):\n",
    "        self.name = name\n",
    "        self.controller_size = int(controller_size)\n",
    "        self.controller_layers = int(controller_layers)\n",
    "        self.num_heads = int(num_heads)\n",
    "        self.sequence_width = int(sequence_width)\n",
    "        self.sequence_min_len = int(sequence_min_len)\n",
    "        self.sequence_max_len = int(sequence_max_len)\n",
    "        self.memory_n = int(memory_n)\n",
    "        self.memory_m = int(memory_m)\n",
    "        self.num_batches = int(num_batches)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.rmsprop_lr = float(rmsprop_lr)\n",
    "        self.rmsprop_momentum = float(rmsprop_momentum)\n",
    "        self.rmsprop_alpha = float(rmsprop_alpha)\n",
    "\n",
    "class CopyTaskModelTraining:\n",
    "    def __init__(self, params=None):\n",
    "        if params is None:\n",
    "            params = CopyTaskParams()\n",
    "        self.params = params\n",
    "        self.net = self.default_net()\n",
    "        self.dataloader = self.default_dataloader()\n",
    "        self.criterion = self.default_criterion()\n",
    "        self.optimizer = self.default_optimizer()\n",
    "\n",
    "    def default_net(self):\n",
    "        net = EncapsulatedNTM(self.params.sequence_width + 1, self.params.sequence_width,\n",
    "                              self.params.controller_size, self.params.controller_layers,\n",
    "                              self.params.num_heads,\n",
    "                              self.params.memory_n, self.params.memory_m)\n",
    "        return net\n",
    "\n",
    "    def default_dataloader(self):\n",
    "        return dataloader(self.params.num_batches, self.params.batch_size,\n",
    "                          self.params.sequence_width,\n",
    "                          self.params.sequence_min_len, self.params.sequence_max_len)\n",
    "\n",
    "    def default_criterion(self):\n",
    "        return nn.BCELoss()\n",
    "\n",
    "    def default_optimizer(self):\n",
    "        return optim.RMSprop(self.net.parameters(),\n",
    "                             momentum=self.params.rmsprop_momentum,\n",
    "                             alpha=self.params.rmsprop_alpha,\n",
    "                             lr=self.params.rmsprop_lr)\n",
    "\n",
    "def dataloader(num_batches, batch_size, seq_width, min_len, max_len):\n",
    "    for batch_num in range(num_batches):\n",
    "        seq_len = random.randint(min_len, max_len)\n",
    "        seq = np.random.binomial(1, 0.5, (seq_len, batch_size, seq_width))\n",
    "        seq = torch.from_numpy(seq)\n",
    "\n",
    "        inp = torch.zeros(seq_len + 1, batch_size, seq_width + 1)\n",
    "        inp[:seq_len, :, :seq_width] = seq\n",
    "        inp[seq_len, :, seq_width] = 1.0\n",
    "        outp = seq.clone()\n",
    "\n",
    "        yield batch_num + 1, inp.float(), outp.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "TASKS = {\n",
    "    'copy': (CopyTaskModelTraining, CopyTaskParams),\n",
    "}\n",
    "\n",
    "RANDOM_SEED = 1000\n",
    "REPORT_INTERVAL = 200\n",
    "CHECKPOINT_INTERVAL = 1000\n",
    "\n",
    "\n",
    "def get_ms():\n",
    "    return time.time() * 1000\n",
    "\n",
    "\n",
    "def init_seed(seed=None):\n",
    "    if seed is None:\n",
    "        seed = int(get_ms() // 1000)\n",
    "\n",
    "    LOGGER.info(\"Using seed=%d\", seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def progress_clean():\n",
    "    print(\"\\r{}\".format(\" \" * 80), end='\\r')\n",
    "\n",
    "\n",
    "def progress_bar(batch_num, report_interval, last_loss):\n",
    "    progress = (((batch_num-1) % report_interval) + 1) / report_interval\n",
    "    fill = int(progress * 40)\n",
    "    print(\"\\r[{}{}]: {} (Loss: {:.4f})\".format(\n",
    "        \"=\" * fill, \" \" * (40 - fill), batch_num, last_loss), end='')\n",
    "\n",
    "\n",
    "def save_checkpoint(net, name, args, batch_num, losses, costs, seq_lengths):\n",
    "    progress_clean()\n",
    "\n",
    "    basename = \"{}/{}-{}-batch-{}\".format(args['checkpoint_path'], name, args['seed'], batch_num)\n",
    "    model_fname = basename + \".model\"\n",
    "    LOGGER.info(\"Saving model checkpoint to: '%s'\", model_fname)\n",
    "    torch.save(net.state_dict(), model_fname)\n",
    "\n",
    "    train_fname = basename + \".json\"\n",
    "    LOGGER.info(\"Saving model training history to '%s'\", train_fname)\n",
    "    content = {\n",
    "        \"loss\": losses,\n",
    "        \"cost\": costs,\n",
    "        \"seq_lengths\": seq_lengths\n",
    "    }\n",
    "    open(train_fname, 'wt').write(json.dumps(content))\n",
    "\n",
    "\n",
    "def clip_grads(net):\n",
    "    parameters = list(filter(lambda p: p.grad is not None, net.parameters()))\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(-10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(net, criterion, optimizer, X, Y):\n",
    "    optimizer.zero_grad()\n",
    "    inp_seq_len = X.size(0)\n",
    "    outp_seq_len, batch_size, _ = Y.size()\n",
    "\n",
    "    net.init_sequence(batch_size)\n",
    "\n",
    "    for i in range(inp_seq_len):\n",
    "        net(X[i])\n",
    "\n",
    "    y_out = torch.zeros(Y.size())\n",
    "    for i in range(outp_seq_len):\n",
    "        y_out[i], _ = net()\n",
    "\n",
    "    loss = criterion(y_out, Y)\n",
    "    loss.backward()\n",
    "    clip_grads(net)\n",
    "    optimizer.step()\n",
    "\n",
    "    y_out_binarized = y_out.clone().data\n",
    "    y_out_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "    cost = torch.sum(torch.abs(y_out_binarized - Y.data))\n",
    "\n",
    "    return loss.item(), cost.item() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, criterion, X, Y):\n",
    "    inp_seq_len = X.size(0)\n",
    "    outp_seq_len, batch_size, _ = Y.size()\n",
    "\n",
    "    net.init_sequence(batch_size)\n",
    "\n",
    "    states = []\n",
    "    for i in range(inp_seq_len):\n",
    "        o, state = net(X[i])\n",
    "        states += [state]\n",
    "\n",
    "    y_out = torch.zeros(Y.size())\n",
    "    for i in range(outp_seq_len):\n",
    "        y_out[i], state = net()\n",
    "        states += [state]\n",
    "\n",
    "    loss = criterion(y_out, Y)\n",
    "\n",
    "    y_out_binarized = y_out.clone().data\n",
    "    y_out_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "    cost = torch.sum(torch.abs(y_out_binarized - Y.data))\n",
    "\n",
    "    result = {\n",
    "        'loss': loss.data[0],\n",
    "        'cost': cost / batch_size,\n",
    "        'y_out': y_out,\n",
    "        'y_out_binarized': y_out_binarized,\n",
    "        'states': states\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, update_list):\n",
    "    for param_str in update_list:\n",
    "        param_split = param_str.split('=')\n",
    "        if len(param_split) != 2:\n",
    "            LOGGER.error(\"Invalid parameter format: '%s'\", param_str)\n",
    "            sys.exit(1)\n",
    "        key, value = param_split\n",
    "        if hasattr(params, key):\n",
    "            setattr(params, key, value)\n",
    "        else:\n",
    "            LOGGER.error(\"Invalid parameter '%s'\", key)\n",
    "            sys.exit(1)\n",
    "    return params\n",
    "\n",
    "def train_model(model, args):\n",
    "    num_batches = model.params.num_batches\n",
    "    batch_size = model.params.batch_size\n",
    "\n",
    "    LOGGER.info(\"Training model for %d batches (batch_size=%d)...\",\n",
    "                num_batches, batch_size)\n",
    "\n",
    "    losses = []\n",
    "    costs = []\n",
    "    seq_lengths = []\n",
    "    start_ms = get_ms()\n",
    "\n",
    "    for batch_num, x, y in model.dataloader:\n",
    "        loss, cost = train_batch(\n",
    "            model.net, model.criterion, model.optimizer, x, y)\n",
    "        losses += [loss]\n",
    "        costs += [cost]\n",
    "        seq_lengths += [y.size(0)]\n",
    "\n",
    "        progress_bar(batch_num, args['report_interval'], loss)\n",
    "\n",
    "        if batch_num % args['report_interval'] == 0:\n",
    "            mean_loss = np.array(losses[-args['report_interval']:]).mean()\n",
    "            mean_cost = np.array(costs[-args['report_interval']:]).mean()\n",
    "            mean_time = int(\n",
    "                ((get_ms() - start_ms) / args['report_interval']) / batch_size)\n",
    "            progress_clean()\n",
    "            LOGGER.info(\"Batch %d Loss: %.6f Cost: %.2f Time: %d ms/sequence\",\n",
    "                        batch_num, mean_loss, mean_cost, mean_time)\n",
    "            start_ms = get_ms()\n",
    "\n",
    "        if (args['checkpoint_interval'] != 0) and (batch_num % args['checkpoint_interval'] == 0):\n",
    "            save_checkpoint(model.net, model.params.name, args,\n",
    "                            batch_num, losses, costs, seq_lengths)\n",
    "\n",
    "    LOGGER.info(\"Done training.\")\n",
    "\n",
    "def init_arguments():\n",
    "    args = {\n",
    "        'seed': RANDOM_SEED,\n",
    "        'task': 'copy',\n",
    "        'param': [],\n",
    "        'checkpoint_interval': CHECKPOINT_INTERVAL,\n",
    "        'checkpoint_path': './',\n",
    "        'report_interval': REPORT_INTERVAL\n",
    "    }\n",
    "\n",
    "    args['checkpoint_path'] = args['checkpoint_path'].rstrip('/')\n",
    "\n",
    "    return args\n",
    "\n",
    "def init_model(args):\n",
    "    LOGGER.info(\"Training for the **%s** task\", args['task'])\n",
    "\n",
    "    model_cls, params_cls = TASKS.get(args['task'], (None, None))\n",
    "\n",
    "    if model_cls is None or params_cls is None:\n",
    "        LOGGER.error(\"Invalid task '%s'\", args['task'])\n",
    "        sys.exit(1)\n",
    "\n",
    "    params = params_cls()\n",
    "    params = update_params(params, args['param'])\n",
    "\n",
    "    LOGGER.info(params)\n",
    "\n",
    "    model = model_cls(params=params)\n",
    "    return model\n",
    "\n",
    "def init_logging():\n",
    "    logging.basicConfig(format='[%(asctime)s] [%(levelname)s] [%(name)s]  %(message)s',\n",
    "                        level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:40:39,907] [INFO] [__main__]  Using seed=1000\n",
      "[2023-09-13 19:40:39,911] [INFO] [__main__]  Training for the **copy** task\n",
      "[2023-09-13 19:40:39,911] [INFO] [__main__]  <__main__.CopyTaskParams object at 0x7f33ab6647d0>\n",
      "[2023-09-13 19:40:39,916] [INFO] [__main__]  Total number of parameters: 62860\n",
      "[2023-09-13 19:40:39,917] [INFO] [__main__]  Training model for 5000 batches (batch_size=1)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 199 (Loss: 0.6739)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:41:12,323] [INFO] [__main__]  Batch 200 Loss: 0.691536 Cost: 38.49 Time: 162 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 399 (Loss: 0.6739)                  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:41:36,559] [INFO] [__main__]  Batch 400 Loss: 0.682120 Cost: 39.23 Time: 121 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 599 (Loss: 0.6547)                  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:42:01,197] [INFO] [__main__]  Batch 600 Loss: 0.669924 Cost: 35.20 Time: 123 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 799 (Loss: 0.6259)                  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:42:28,596] [INFO] [__main__]  Batch 800 Loss: 0.665752 Cost: 38.42 Time: 136 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 999 (Loss: 0.6966)                  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:42:57,612] [INFO] [__main__]  Batch 1000 Loss: 0.656061 Cost: 36.27 Time: 145 ms/sequence\n",
      "[2023-09-13 19:42:57,613] [INFO] [__main__]  Saving model checkpoint to: './copy-task-1000-batch-1000.model'\n",
      "[2023-09-13 19:42:57,617] [INFO] [__main__]  Saving model training history to './copy-task-1000-batch-1000.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 1198 (Loss: 0.6817)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:43:25,714] [INFO] [__main__]  Batch 1200 Loss: 0.654393 Cost: 36.38 Time: 140 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 1398 (Loss: 0.6405)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:43:54,329] [INFO] [__main__]  Batch 1400 Loss: 0.647553 Cost: 36.85 Time: 143 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 1599 (Loss: 0.6812)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:44:21,010] [INFO] [__main__]  Batch 1600 Loss: 0.641334 Cost: 34.49 Time: 133 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 1799 (Loss: 0.5670)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:44:47,598] [INFO] [__main__]  Batch 1800 Loss: 0.633838 Cost: 35.28 Time: 132 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 1998 (Loss: 0.6740)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:45:14,010] [INFO] [__main__]  Batch 2000 Loss: 0.628209 Cost: 33.15 Time: 132 ms/sequence\n",
      "[2023-09-13 19:45:14,012] [INFO] [__main__]  Saving model checkpoint to: './copy-task-1000-batch-2000.model'\n",
      "[2023-09-13 19:45:14,015] [INFO] [__main__]  Saving model training history to './copy-task-1000-batch-2000.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 2198 (Loss: 0.6788)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:45:38,107] [INFO] [__main__]  Batch 2200 Loss: 0.619515 Cost: 33.32 Time: 120 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 2398 (Loss: 0.6670)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:46:02,813] [INFO] [__main__]  Batch 2400 Loss: 0.621100 Cost: 32.38 Time: 123 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 2599 (Loss: 0.6821)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:46:29,437] [INFO] [__main__]  Batch 2600 Loss: 0.607359 Cost: 31.95 Time: 133 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 2799 (Loss: 0.6902)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:46:56,223] [INFO] [__main__]  Batch 2800 Loss: 0.611724 Cost: 35.51 Time: 133 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 2998 (Loss: 0.1753)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:47:22,822] [INFO] [__main__]  Batch 3000 Loss: 0.601232 Cost: 33.44 Time: 132 ms/sequence\n",
      "[2023-09-13 19:47:22,824] [INFO] [__main__]  Saving model checkpoint to: './copy-task-1000-batch-3000.model'\n",
      "[2023-09-13 19:47:22,828] [INFO] [__main__]  Saving model training history to './copy-task-1000-batch-3000.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 3197 (Loss: 0.6291)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:47:49,429] [INFO] [__main__]  Batch 3200 Loss: 0.599935 Cost: 33.64 Time: 133 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 3398 (Loss: 0.6271)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:48:16,318] [INFO] [__main__]  Batch 3400 Loss: 0.602553 Cost: 34.11 Time: 134 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 3599 (Loss: 0.5582)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:48:42,088] [INFO] [__main__]  Batch 3600 Loss: 0.596330 Cost: 33.89 Time: 128 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 3799 (Loss: 0.0977)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:49:07,895] [INFO] [__main__]  Batch 3800 Loss: 0.581937 Cost: 34.45 Time: 129 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 3996 (Loss: 0.0942)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:49:36,128] [INFO] [__main__]  Batch 4000 Loss: 0.581719 Cost: 32.81 Time: 141 ms/sequence\n",
      "[2023-09-13 19:49:36,129] [INFO] [__main__]  Saving model checkpoint to: './copy-task-1000-batch-4000.model'\n",
      "[2023-09-13 19:49:36,133] [INFO] [__main__]  Saving model training history to './copy-task-1000-batch-4000.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 4199 (Loss: 0.6455)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:50:02,966] [INFO] [__main__]  Batch 4200 Loss: 0.593039 Cost: 34.19 Time: 134 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 4399 (Loss: 0.5906)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:50:28,730] [INFO] [__main__]  Batch 4400 Loss: 0.575995 Cost: 32.62 Time: 128 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 4599 (Loss: 0.6371)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:50:55,571] [INFO] [__main__]  Batch 4600 Loss: 0.584684 Cost: 33.38 Time: 134 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 4798 (Loss: 0.6520)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:51:23,134] [INFO] [__main__]  Batch 4800 Loss: 0.586175 Cost: 33.01 Time: 137 ms/sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ]: 4998 (Loss: 0.5667)                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-13 19:51:51,000] [INFO] [__main__]  Batch 5000 Loss: 0.572626 Cost: 33.08 Time: 139 ms/sequence\n",
      "[2023-09-13 19:51:51,002] [INFO] [__main__]  Saving model checkpoint to: './copy-task-1000-batch-5000.model'\n",
      "[2023-09-13 19:51:51,006] [INFO] [__main__]  Saving model training history to './copy-task-1000-batch-5000.json'\n",
      "[2023-09-13 19:51:51,016] [INFO] [__main__]  Done training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    init_logging()\n",
    "\n",
    "    args = init_arguments()\n",
    "\n",
    "    init_seed(args['seed'])\n",
    "\n",
    "    model = init_model(args)\n",
    "\n",
    "    LOGGER.info(\"Total number of parameters: %d\",\n",
    "                model.net.calculate_num_params())\n",
    "    train_model(model, args)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f94c6b32fbda5dcd64daf382f382d2d5da78e483f351d87e126340144fcf47d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
