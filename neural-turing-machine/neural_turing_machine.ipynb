{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Neural Turing Machines\" paper implementation - https://arxiv.org/pdf/1410.5401.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from model.model import EncapsulatedNTM\n",
    "\n",
    "class CopyTaskParams:\n",
    "    def __init__(self, name=\"copy-task\", controller_size=100, controller_layers=1, num_heads=1,\n",
    "                 sequence_width=8, sequence_min_len=1, sequence_max_len=20, memory_n=128, memory_m=20,\n",
    "                 num_batches=50000, batch_size=1, rmsprop_lr=1e-4, rmsprop_momentum=0.9, rmsprop_alpha=0.95):\n",
    "        self.name = name\n",
    "        self.controller_size = int(controller_size)\n",
    "        self.controller_layers = int(controller_layers)\n",
    "        self.num_heads = int(num_heads)\n",
    "        self.sequence_width = int(sequence_width)\n",
    "        self.sequence_min_len = int(sequence_min_len)\n",
    "        self.sequence_max_len = int(sequence_max_len)\n",
    "        self.memory_n = int(memory_n)\n",
    "        self.memory_m = int(memory_m)\n",
    "        self.num_batches = int(num_batches)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.rmsprop_lr = float(rmsprop_lr)\n",
    "        self.rmsprop_momentum = float(rmsprop_momentum)\n",
    "        self.rmsprop_alpha = float(rmsprop_alpha)\n",
    "\n",
    "class CopyTaskModelTraining:\n",
    "    def __init__(self, params=None):\n",
    "        if params is None:\n",
    "            params = CopyTaskParams()\n",
    "        self.params = params\n",
    "        self.net = self.default_net()\n",
    "        self.dataloader = self.default_dataloader()\n",
    "        self.criterion = self.default_criterion()\n",
    "        self.optimizer = self.default_optimizer()\n",
    "\n",
    "    def default_net(self):\n",
    "        net = EncapsulatedNTM(self.params.sequence_width + 1, self.params.sequence_width,\n",
    "                              self.params.controller_size, self.params.controller_layers,\n",
    "                              self.params.num_heads,\n",
    "                              self.params.memory_n, self.params.memory_m)\n",
    "        return net\n",
    "\n",
    "    def default_dataloader(self):\n",
    "        return dataloader(self.params.num_batches, self.params.batch_size,\n",
    "                          self.params.sequence_width,\n",
    "                          self.params.sequence_min_len, self.params.sequence_max_len)\n",
    "\n",
    "    def default_criterion(self):\n",
    "        return nn.BCELoss()\n",
    "\n",
    "    def default_optimizer(self):\n",
    "        return optim.RMSprop(self.net.parameters(),\n",
    "                             momentum=self.params.rmsprop_momentum,\n",
    "                             alpha=self.params.rmsprop_alpha,\n",
    "                             lr=self.params.rmsprop_lr)\n",
    "\n",
    "def dataloader(num_batches, batch_size, seq_width, min_len, max_len):\n",
    "    for batch_num in range(num_batches):\n",
    "        seq_len = random.randint(min_len, max_len)\n",
    "        seq = np.random.binomial(1, 0.5, (seq_len, batch_size, seq_width))\n",
    "        seq = torch.from_numpy(seq)\n",
    "\n",
    "        inp = torch.zeros(seq_len + 1, batch_size, seq_width + 1)\n",
    "        inp[:seq_len, :, :seq_width] = seq\n",
    "        inp[seq_len, :, seq_width] = 1.0\n",
    "        outp = seq.clone()\n",
    "\n",
    "        yield batch_num + 1, inp.float(), outp.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import attr\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "TASKS = {\n",
    "    'copy': (CopyTaskModelTraining, CopyTaskParams),\n",
    "}\n",
    "\n",
    "RANDOM_SEED = 1000\n",
    "REPORT_INTERVAL = 200\n",
    "CHECKPOINT_INTERVAL = 1000\n",
    "\n",
    "\n",
    "def get_ms():\n",
    "    return time.time() * 1000\n",
    "\n",
    "\n",
    "def init_seed(seed=None):\n",
    "    if seed is None:\n",
    "        seed = int(get_ms() // 1000)\n",
    "\n",
    "    LOGGER.info(\"Using seed=%d\", seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def progress_clean():\n",
    "    print(\"\\r{}\".format(\" \" * 80), end='\\r')\n",
    "\n",
    "\n",
    "def progress_bar(batch_num, report_interval, last_loss):\n",
    "    progress = (((batch_num-1) % report_interval) + 1) / report_interval\n",
    "    fill = int(progress * 40)\n",
    "    print(\"\\r[{}{}]: {} (Loss: {:.4f})\".format(\n",
    "        \"=\" * fill, \" \" * (40 - fill), batch_num, last_loss), end='')\n",
    "\n",
    "\n",
    "def save_checkpoint(net, name, args, batch_num, losses, costs, seq_lengths):\n",
    "    progress_clean()\n",
    "\n",
    "    basename = \"{}/{}-{}-batch-{}\".format(args.checkpoint_path, name, args.seed, batch_num)\n",
    "    model_fname = basename + \".model\"\n",
    "    LOGGER.info(\"Saving model checkpoint to: '%s'\", model_fname)\n",
    "    torch.save(net.state_dict(), model_fname)\n",
    "\n",
    "    train_fname = basename + \".json\"\n",
    "    LOGGER.info(\"Saving model training history to '%s'\", train_fname)\n",
    "    content = {\n",
    "        \"loss\": losses,\n",
    "        \"cost\": costs,\n",
    "        \"seq_lengths\": seq_lengths\n",
    "    }\n",
    "    open(train_fname, 'wt').write(json.dumps(content))\n",
    "\n",
    "\n",
    "def clip_grads(net):\n",
    "    parameters = list(filter(lambda p: p.grad is not None, net.parameters()))\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(-10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(net, criterion, optimizer, X, Y):\n",
    "    optimizer.zero_grad()\n",
    "    inp_seq_len = X.size(0)\n",
    "    outp_seq_len, batch_size, _ = Y.size()\n",
    "\n",
    "    net.init_sequence(batch_size)\n",
    "\n",
    "    for i in range(inp_seq_len):\n",
    "        net(X[i])\n",
    "\n",
    "    y_out = torch.zeros(Y.size())\n",
    "    for i in range(outp_seq_len):\n",
    "        y_out[i], _ = net()\n",
    "\n",
    "    loss = criterion(y_out, Y)\n",
    "    loss.backward()\n",
    "    clip_grads(net)\n",
    "    optimizer.step()\n",
    "\n",
    "    y_out_binarized = y_out.clone().data\n",
    "    y_out_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "    cost = torch.sum(torch.abs(y_out_binarized - Y.data))\n",
    "\n",
    "    return loss.item(), cost.item() / batch_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f94c6b32fbda5dcd64daf382f382d2d5da78e483f351d87e126340144fcf47d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
