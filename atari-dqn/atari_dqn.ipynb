{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Playing Atari with Deep Reinforcement Learning\" paper implementation - https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petko/projects/ai-notebooks/env/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/tmp/ipykernel_2951/3946091196.py:18: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from IPython import display\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import rescale\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, num_frames, num_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.num_frames = num_frames\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_frames, out_channels=16, kernel_size=8, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=3200, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "update_interval = 4\n",
    "clone_interval = int(1e4)\n",
    "save_interval = int(1e5)\n",
    "frame_skip = None\n",
    "num_frames = 4\n",
    "num_actions = 4\n",
    "episodes = int(1e5)\n",
    "memory_depth = int(1e5)\n",
    "epsilon_i = 1.0\n",
    "epsilon_f = 0.1\n",
    "anneal_time = int(1e6)\n",
    "burn_in = int(5e4)\n",
    "gamma = 0.99\n",
    "learning_rate = 2.5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'terminal', 'next_state'])\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model, memory_depth, lr, gamma, epsilon_i, epsilon_f, anneal_time, ckptdir):\n",
    "        \n",
    "        self.cuda = True if torch.cuda.is_available() else False\n",
    "        \n",
    "        self.model = model\n",
    "        self.device = torch.device(\"cuda\" if self.cuda else \"cpu\")\n",
    "        \n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        \n",
    "        self.memory_depth = memory_depth\n",
    "        self.gamma = torch.tensor([gamma], device=self.device)\n",
    "        self.e_i = epsilon_i\n",
    "        self.e_f = epsilon_f\n",
    "        self.anneal_time = anneal_time\n",
    "        self.ckptdir = ckptdir\n",
    "        \n",
    "        if not os.path.isdir(ckptdir):\n",
    "            os.makedirs(ckptdir)\n",
    "        \n",
    "        self.memory = deque(maxlen=memory_depth)\n",
    "        self.clone()\n",
    "        \n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "        self.opt = torch.optim.RMSprop(self.model.parameters(), lr=lr, alpha=0.95, eps=0.01)\n",
    "        \n",
    "    def clone(self):\n",
    "        try:\n",
    "            del self.clone_model\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.clone_model = copy.deepcopy(self.model)\n",
    "        \n",
    "        for p in self.clone_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        if self.cuda:\n",
    "            self.clone_model = self.clone_model.cuda()\n",
    "    \n",
    "    def remember(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def retrieve(self, batch_size):\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state, action, reward, terminal, next_state = map(torch.cat, [*batch])\n",
    "        return state, action, reward, terminal, next_state\n",
    "    \n",
    "    @property\n",
    "    def memories(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def act(self, state):\n",
    "        q_values = self.model(state).detach()\n",
    "        action = torch.argmax(q_values)\n",
    "        return action.item()\n",
    "    \n",
    "    def process(self, state):\n",
    "        state = rgb2gray(state[35:195, :, :])\n",
    "        state = rescale(state, scale=0.5)\n",
    "        state = state[np.newaxis, np.newaxis, :, :]\n",
    "        return torch.tensor(state, device=self.device, dtype=torch.float)\n",
    "    \n",
    "    def exploration_rate(self, t):\n",
    "        if 0 <= t < self.anneal_time:\n",
    "            return self.e_i - t*(self.e_i - self.e_f)/self.anneal_time\n",
    "        elif t >= self.anneal_time:\n",
    "            return self.e_f\n",
    "        elif t < 0:\n",
    "            return self.e_i\n",
    "    \n",
    "    def save(self, t):\n",
    "        save_path = os.path.join(self.ckptdir, 'model-{}'.format(t))\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "    \n",
    "    def load(self):\n",
    "        ckpts = [file for file in os.listdir(self.ckptdir) if 'model' in file]\n",
    "        \n",
    "        if not ckpts:\n",
    "            print(\"No checkpoint files found in {}\".format(self.ckptdir))\n",
    "            return\n",
    "        \n",
    "        steps = [int(re.search('\\d+', file).group(0)) for file in ckpts]\n",
    "        \n",
    "        latest_ckpt = ckpts[np.argmax(steps)]\n",
    "        self.t = np.max(steps)\n",
    "        \n",
    "        print(\"Loading checkpoint: {}\".format(latest_ckpt))\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(os.path.join(self.ckptdir, latest_ckpt)))\n",
    "        \n",
    "    def update(self, batch_size):\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        state, action, reward, terminal, next_state = self.retrieve(batch_size)\n",
    "        q = self.model(state).gather(1, action.view(batch_size, 1))\n",
    "        qmax = self.clone_model(next_state).max(dim=1)[0]\n",
    "        \n",
    "        nonterminal_target = reward + self.gamma*qmax\n",
    "        terminal_target = reward\n",
    "        \n",
    "        target = terminal.float()*terminal_target + (~terminal).float()*nonterminal_target\n",
    "    \n",
    "        loss = self.loss(q.view(-1), target)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "    def play(self, episodes, train=False, load=False, plot=False, render=False, verbose=False):\n",
    "    \n",
    "        self.t = 0\n",
    "        metadata = dict(episode=[], reward=[])\n",
    "        \n",
    "        if load:\n",
    "            self.load()\n",
    "\n",
    "        try:\n",
    "            progress_bar = range(episodes)\n",
    "            \n",
    "            i = 0\n",
    "            for episode in progress_bar:\n",
    "\n",
    "                state = env.reset()[0]\n",
    "                state = self.process(state)\n",
    "                \n",
    "                done = False\n",
    "                total_reward = 0\n",
    "\n",
    "                while not done:\n",
    "\n",
    "                    if render:\n",
    "                        env.render()\n",
    "\n",
    "                    while state.size()[1] < num_frames:\n",
    "                        action = 1\n",
    "\n",
    "                        new_frame, reward, done, truncated, info = env.step(action)\n",
    "                        new_frame = self.process(new_frame)\n",
    "\n",
    "                        state = torch.cat([state, new_frame], 1)\n",
    "                    \n",
    "                    if train and np.random.uniform() < self.exploration_rate(self.t-burn_in):\n",
    "                        action = np.random.choice(num_actions)\n",
    "\n",
    "                    else:\n",
    "                        action = self.act(state)\n",
    "\n",
    "                    new_frame, reward, done, truncated, info = env.step(action)\n",
    "                    new_frame = self.process(new_frame)\n",
    "\n",
    "                    new_state = torch.cat([state, new_frame], 1)\n",
    "                    new_state = new_state[:, 1:, :, :]\n",
    "                    \n",
    "                    if train:\n",
    "                        reward = torch.tensor([reward], device=self.device, dtype=torch.float)\n",
    "                        action = torch.tensor([action], device=self.device, dtype=torch.long)\n",
    "                        done = torch.tensor([done], device=self.device, dtype=torch.uint8)\n",
    "                        \n",
    "                        self.remember(state, action, reward, done, new_state)\n",
    "\n",
    "                    state = new_state\n",
    "                    total_reward += reward\n",
    "                    self.t += 1\n",
    "                    i += 1\n",
    "                    \n",
    "                    if not train:\n",
    "                        time.sleep(0.1)\n",
    "                    \n",
    "                    if train and self.t > burn_in and i > batch_size:\n",
    "\n",
    "                        if self.t % update_interval == 0:\n",
    "                            self.update(batch_size)\n",
    "\n",
    "                        if self.t % clone_interval == 0:\n",
    "                            self.clone()\n",
    "\n",
    "                        if self.t % save_interval == 0:\n",
    "                            self.save(self.t)\n",
    "\n",
    "                metadata['episode'].append(episode)\n",
    "                metadata['reward'].append(total_reward)\n",
    "\n",
    "                if episode % 100 == 0 and episode != 0:\n",
    "                    filtered_rewards = [float(value) for value in metadata['reward'] if isinstance(value, (int, float))]\n",
    "\n",
    "                    if len(filtered_rewards) >= 100:\n",
    "                        avg_return = np.mean(filtered_rewards[-100:])\n",
    "                        print(\"Average return (last 100 episodes): {:.2f}\".format(avg_return))\n",
    "                    else:\n",
    "                        print(\"Not enough data to compute average return.\")\n",
    "\n",
    "                if plot:\n",
    "                    plt.scatter(metadata['episode'], metadata['reward'])\n",
    "                    plt.xlim(0, episodes)\n",
    "                    plt.xlabel(\"Episode\")\n",
    "                    plt.ylabel(\"Return\")\n",
    "                    display.clear_output(wait=True)\n",
    "                    display.display(plt.gcf())\n",
    "            \n",
    "            env.close()\n",
    "            return metadata\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            if train:\n",
    "                print(\"Saving model before quitting...\")\n",
    "                self.save(self.t)\n",
    "            \n",
    "            env.close()\n",
    "            return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepQNetwork(num_frames, num_actions)\n",
    "\n",
    "agent = Agent(model, memory_depth, learning_rate, gamma, epsilon_i, epsilon_f, anneal_time, './ckpt')\n",
    "\n",
    "metadata = agent.play(episodes, train=True, load=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f94c6b32fbda5dcd64daf382f382d2d5da78e483f351d87e126340144fcf47d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
