{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Sequence to Sequence Learning with Neural Networks\" paper implementation - https://arxiv.org/pdf/1409.3215.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from data_loader import Dataset\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11793 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3117\n",
      "eng 3117\n",
      "['you re disobeying orders ', 'you re disobeying orders ']\n",
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11793 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3117\n",
      "eng 3117\n",
      "['he s an aristocrat ', 'he s an aristocrat ']\n"
     ]
    }
   ],
   "source": [
    "SOS_token = 1 \n",
    "EOS_token = 2 \n",
    "\n",
    "args = {\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,\n",
    "    'gamma': 0.1,\n",
    "    'epochs_per_lr_drop': 450,\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 8,\n",
    "    'num_epoch': 600,\n",
    "    'cuda': True,\n",
    "    'save_folder': os.path.expanduser('~/weights'),\n",
    "    'epochs_per_save': 10,\n",
    "    'batch_per_log': 10,\n",
    "    'auto_encoder': True,\n",
    "    'MAX_LENGTH': 10,\n",
    "    'bidirectional': False,\n",
    "    'hidden_size_decoder': 256,\n",
    "    'num_layer_decoder': 1,\n",
    "    'hidden_size_encoder': 256,\n",
    "    'num_layer_encoder': 1,\n",
    "    'teacher_forcing': False\n",
    "}\n",
    "\n",
    "if args['cuda']:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "trainset = Dataset(phase='train', max_input_length=10, auto_encoder=args['auto_encoder'])\n",
    "\n",
    "input_lang, output_lang = trainset.langs()\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=args['batch_size'],\n",
    "                                          shuffle=True, num_workers=args['num_workers'], pin_memory=False, drop_last=True)\n",
    "dataiter = iter(trainloader)\n",
    "\n",
    "testset = Dataset(phase='test', max_input_length=10, auto_encoder=args['auto_encoder'])\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                          shuffle=True, num_workers=1, pin_memory=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size, batch_size, num_layers=1, bidirectional=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim=hidden_size)\n",
    "\n",
    "        if args['bidirectional']:\n",
    "            self.lstm_forward = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "            self.lstm_backward = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        if args['bidirectional']:\n",
    "            input_forward, input_backward = input\n",
    "            hidden_forward, hidden_backward = hidden\n",
    "            input_forward = self.embedding(input_forward).view(1, 1, -1)\n",
    "            input_backward = self.embedding(input_backward).view(1, 1, -1)\n",
    "\n",
    "            out_forward, (h_n_forward, c_n_forward) = self.lstm_forward(input_forward, hidden_forward)\n",
    "            out_backward, (h_n_backward, c_n_backward) = self.lstm_backward(input_backward, hidden_backward)\n",
    "\n",
    "            forward_state = (h_n_forward, c_n_forward)\n",
    "            backward_state = (h_n_backward, c_n_backward)\n",
    "            output_state = (forward_state, backward_state)\n",
    "\n",
    "            return output_state\n",
    "        else:\n",
    "            embedded = self.embedding(input).view(1, 1, -1)\n",
    "            rnn_input = embedded\n",
    "            output, (h_n, c_n) = self.lstm(rnn_input, hidden)\n",
    "            return output, (h_n, c_n)\n",
    "\n",
    "    def init_hidden(self):\n",
    "\n",
    "        if self.bidirectional:\n",
    "            encoder_state = [torch.zeros(self.num_layers, 1, self.hidden_size, device=device),\n",
    "                                      torch.zeros(self.num_layers, 1, self.hidden_size, device=device)]\n",
    "            encoder_state = {\"forward\": encoder_state, \"backward\": encoder_state}\n",
    "            return encoder_state\n",
    "        else:\n",
    "            encoder_state = [torch.zeros(self.num_layers, 1, self.hidden_size, device=device),\n",
    "                              torch.zeros(self.num_layers, 1, self.hidden_size, device=device)]\n",
    "            return encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size,\n",
    "                            hidden_size=hidden_size, num_layers=1)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output, (h_n, c_n) = self.lstm(output, hidden)\n",
    "        output = self.out(output[0])\n",
    "        return output, (h_n, c_n)\n",
    "\n",
    "    def initHidden(self):\n",
    "        return [torch.zeros(self.num_layers, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(self.num_layers, 1, self.hidden_size, device=device)]\n",
    "\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, bidirectional, hidden_size_encoder, hidden_size_decoder):\n",
    "        super(Linear, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        num_directions = int(bidirectional) + 1\n",
    "        self.linear_connection_op = nn.Linear(\n",
    "            num_directions * hidden_size_encoder, hidden_size_decoder)\n",
    "        self.connection_possibility_status = num_directions * \\\n",
    "            hidden_size_encoder == hidden_size_decoder\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if self.connection_possibility_status:\n",
    "            return input\n",
    "        else:\n",
    "            return self.linear_connection_op(input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f94c6b32fbda5dcd64daf382f382d2d5da78e483f351d87e126340144fcf47d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
