{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Asynchronous Methods for Deep Reinforcement Learning\" paper implementation - https://arxiv.org/pdf/1602.01783.pdf\n",
    "\n",
    "## This implementation includes only the Actor-Critic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from environments import create_atari_env\n",
    "from model import ActorCritic\n",
    "\n",
    "\n",
    "def ensure_shared_grads(model, shared_model):\n",
    "    for param, shared_param in zip(model.parameters(),\n",
    "                                   shared_model.parameters()):\n",
    "        if shared_param.grad is not None:\n",
    "            return\n",
    "        shared_param._grad = param.grad\n",
    "\n",
    "\n",
    "def train(rank, args, shared_model, counter, lock, optimizer=None):\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "\n",
    "    env = create_atari_env(args.env_name)\n",
    "    env.seed(args.seed + rank)\n",
    "\n",
    "    model = ActorCritic(env.observation_space.shape[0], env.action_space)\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.Adam(shared_model.parameters(), lr=args.lr)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state)\n",
    "    done = True\n",
    "\n",
    "    episode_length = 0\n",
    "    while True:\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "        if done:\n",
    "            cx = torch.zeros(1, 256)\n",
    "            hx = torch.zeros(1, 256)\n",
    "        else:\n",
    "            cx = cx.detach()\n",
    "            hx = hx.detach()\n",
    "\n",
    "        values = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        entropies = []\n",
    "\n",
    "        for step in range(args.num_steps):\n",
    "            episode_length += 1\n",
    "            value, logit, (hx, cx) = model((state.unsqueeze(0),\n",
    "                                            (hx, cx)))\n",
    "            prob = F.softmax(logit, dim=-1)\n",
    "            log_prob = F.log_softmax(logit, dim=-1)\n",
    "            entropy = -(log_prob * prob).sum(1, keepdim=True)\n",
    "            entropies.append(entropy)\n",
    "\n",
    "            action = prob.multinomial(num_samples=1).detach()\n",
    "            log_prob = log_prob.gather(1, action)\n",
    "\n",
    "            state, reward, done, _ = env.step(action.numpy())\n",
    "            done = done or episode_length >= args.max_episode_length\n",
    "            reward = max(min(reward, 1), -1)\n",
    "\n",
    "            with lock:\n",
    "                counter.value += 1\n",
    "\n",
    "            if done:\n",
    "                episode_length = 0\n",
    "                state = env.reset()\n",
    "\n",
    "            state = torch.from_numpy(state)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        R = torch.zeros(1, 1)\n",
    "        if not done:\n",
    "            value, _, _ = model((state.unsqueeze(0), (hx, cx)))\n",
    "            R = value.detach()\n",
    "\n",
    "        values.append(R)\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        gae = torch.zeros(1, 1)\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = args.gamma * R + rewards[i]\n",
    "            advantage = R - values[i]\n",
    "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
    "\n",
    "            delta_t = rewards[i] + args.gamma * values[i + 1] - values[i]\n",
    "            gae = gae * args.gamma * args.gae_lambda + delta_t\n",
    "\n",
    "            policy_loss = policy_loss - \\\n",
    "                log_probs[i] * gae.detach() - args.entropy_coef * entropies[i]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        (policy_loss + args.value_loss_coef * value_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "        ensure_shared_grads(model, shared_model)\n",
    "        optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f94c6b32fbda5dcd64daf382f382d2d5da78e483f351d87e126340144fcf47d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
