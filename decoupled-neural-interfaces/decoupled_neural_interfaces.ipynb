{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Decoupled Neural Interfaces using Synthetic Gradients\" paper implementation - https://arxiv.org/pdf/1608.05343.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class mnist():\n",
    "    def __init__(self, args):\n",
    "        train_dataset = dsets.MNIST(root='./data',\n",
    "                                    train=True,\n",
    "                                    transform=transforms.ToTensor(),\n",
    "                                    download=True)\n",
    "\n",
    "        test_dataset = dsets.MNIST(root='./data',\n",
    "                                   train=False,\n",
    "                                   transform=transforms.ToTensor())\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                        batch_size=args['batch_size'],\n",
    "                                                        shuffle=True)\n",
    "\n",
    "        self.test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                                       batch_size=args['batch_size'],\n",
    "                                                       shuffle=False)\n",
    "        self.input_dims = 784\n",
    "        self.num_classes = 10\n",
    "        self.in_channel = 1\n",
    "        self.num_train = len(train_dataset)\n",
    "\n",
    "\n",
    "class cifar10():\n",
    "    def __init__(self, args):\n",
    "        transform = self.image_transform()\n",
    "        train_dataset = dsets.CIFAR10(root='./data/',\n",
    "                                      train=True,\n",
    "                                      transform=transform,\n",
    "                                      download=True)\n",
    "\n",
    "        test_dataset = dsets.CIFAR10(root='./data/',\n",
    "                                     train=False,\n",
    "                                     transform=transforms.ToTensor())\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                        batch_size=100,\n",
    "                                                        shuffle=True)\n",
    "\n",
    "        self.test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                                       batch_size=100,\n",
    "                                                       shuffle=False)\n",
    "        self.num_classes = 10\n",
    "        self.in_channel = 3\n",
    "        self.num_train = len(train_dataset)\n",
    "\n",
    "    def image_transform(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(40),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(28),\n",
    "            transforms.ToTensor()])\n",
    "        return transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class dni_linear(nn.Module):\n",
    "    def __init__(self, input_dims, num_classes, dni_hidden_size=1024, conditioned=False):\n",
    "        super(dni_linear, self).__init__()\n",
    "        self.conditioned = conditioned\n",
    "        if self.conditioned:\n",
    "            dni_input_dims = input_dims+num_classes\n",
    "        else:\n",
    "            dni_input_dims = input_dims\n",
    "        self.layer1 = nn.Sequential(\n",
    "                      nn.Linear(dni_input_dims, dni_hidden_size),\n",
    "                      nn.BatchNorm1d(dni_hidden_size),\n",
    "                      nn.ReLU()\n",
    "                      )\n",
    "        self.layer2 = nn.Sequential(\n",
    "                      nn.Linear(dni_hidden_size, dni_hidden_size),\n",
    "                      nn.BatchNorm1d(dni_hidden_size),\n",
    "                      nn.ReLU()\n",
    "                      )\n",
    "        self.layer3 = nn.Linear(dni_hidden_size, input_dims)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        if self.conditioned:\n",
    "            assert y is not None\n",
    "            x = torch.cat((x, y), 1)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        return out\n",
    "\n",
    "class dni_Conv2d(nn.Module):\n",
    "    def __init__(self, input_dims, input_size, num_classes, dni_hidden_size=64, conditioned=False):\n",
    "        super(dni_Conv2d, self).__init__()\n",
    "        self.conditioned = conditioned\n",
    "        if self.conditioned:\n",
    "            dni_input_dims = input_dims+1\n",
    "        else:\n",
    "            dni_input_dims = input_dims\n",
    "\n",
    "        self.input_size = list(input_size)\n",
    "        self.label_emb = nn.Linear(num_classes, np.prod(np.array(input_size)))\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "                      nn.Conv2d(dni_input_dims, dni_hidden_size, kernel_size=5, padding=2),\n",
    "                      nn.BatchNorm2d(dni_hidden_size),\n",
    "                      nn.ReLU())\n",
    "        self.layer2 = nn.Sequential( \n",
    "                      nn.Conv2d(dni_hidden_size, dni_hidden_size, kernel_size=5, padding=2),\n",
    "                      nn.BatchNorm2d(dni_hidden_size),\n",
    "                      nn.ReLU())\n",
    "        self.layer3 = nn.Sequential(\n",
    "                      nn.Conv2d(dni_hidden_size, input_dims, kernel_size=5, padding=2))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        if self.conditioned:\n",
    "            assert y is not None\n",
    "            y = self.label_emb(y)\n",
    "            y = y.view([-1, 1]+self.input_size)\n",
    "            x = torch.cat((x, y), 1)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class cnn(nn.Module):\n",
    "    def __init__(self, in_channel, conditioned_DNI, num_classes):\n",
    "        super(cnn, self).__init__()\n",
    "\n",
    "               \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "\n",
    "        # DNI module\n",
    "        self._layer1 = dni_Conv2d(16, (14, 14), num_classes, conditioned=conditioned_DNI)\n",
    "        self._layer2 = dni_Conv2d(32, (7, 7), num_classes, conditioned=conditioned_DNI)\n",
    "        self._fc = dni_linear(num_classes, num_classes, conditioned=conditioned_DNI)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "                   self.layer1, \n",
    "                   self.layer2, \n",
    "                   self.fc)\n",
    "        self.dni = nn.Sequential(\n",
    "                   self._layer1, \n",
    "                   self._layer2, \n",
    "                   self._fc)\n",
    "        self.optimizers = []\n",
    "        self.forwards = []\n",
    "        self.init_optimzers()\n",
    "        self.init_forwards()\n",
    "\n",
    "    def init_optimzers(self, learning_rate=0.001):\n",
    "        self.optimizers.append(torch.optim.Adam(self.layer1.parameters(), lr=learning_rate))\n",
    "        self.optimizers.append(torch.optim.Adam(self.layer2.parameters(), lr=learning_rate))\n",
    "        self.optimizers.append(torch.optim.Adam(self.fc.parameters(), lr=learning_rate))\n",
    "        self.optimizer = torch.optim.Adam(self.cnn.parameters(), lr=learning_rate)\n",
    "        self.grad_optimizer = torch.optim.Adam(self.dni.parameters(), lr=learning_rate)\n",
    "\n",
    "    def init_forwards(self):\n",
    "        self.forwards.append(self.forward_layer1)\n",
    "        self.forwards.append(self.forward_layer2)\n",
    "        self.forwards.append(self.forward_fc)\n",
    "\n",
    "    def forward_layer1(self, x, y=None):\n",
    "        out = self.layer1(x)\n",
    "        grad = self._layer1(out, y)\n",
    "        return out, grad\n",
    " \n",
    "    def forward_layer2(self, x, y=None):\n",
    "        out = self.layer2(x)\n",
    "        grad = self._layer2(out, y)\n",
    "        return out, grad\n",
    "    \n",
    "    def forward_fc(self, x, y=None):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        grad = self._fc(out, y)\n",
    "        return out, grad\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        layer1 = self.layer1(x)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer2_flat = layer2.view(layer2.size(0), -1)\n",
    "        fc = self.fc(layer2_flat)\n",
    "        if y is not None:\n",
    "            grad_layer1 = self._layer1(layer1, y)\n",
    "            grad_layer2 = self._layer2(layer2, y)\n",
    "            grad_fc = self._fc(fc, y)\n",
    "            return (layer1, layer2, fc), (grad_layer1, grad_layer2, grad_fc)\n",
    "        else:\n",
    "            return layer1, layer2, fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "def plot(data=None, path=None, name='DNI'):\n",
    "    assert data is not None or path is not None\n",
    "    if path is not None:\n",
    "        data = pkl.load(open(path))\n",
    "    \n",
    "    grad_loss = data['grad_loss']\n",
    "    classify_loss = data['classify_loss']\n",
    "    x = np.arange(len(classify_loss))\n",
    "\n",
    "    plt.plot(classify_loss)\n",
    "    plt.title('classify_loss')\n",
    "    plt.savefig(name+'_classify_loss.png')    \n",
    "    plt.close()\n",
    "\n",
    "    plt.semilogy(grad_loss, 'r')\n",
    "    plt.title('grad_loss')\n",
    "    plt.savefig(name+'_grad_loss.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class classifier():\n",
    "    def __init__(self, args, data):\n",
    "        self.train_loader = data.train_loader\n",
    "        self.test_loader = data.test_loader\n",
    "        self.batch_size = args['batch_size']\n",
    "        self.num_train = data.num_train\n",
    "        self.num_classes = data.num_classes\n",
    "        self.net = cnn(data.in_channel, args['conditioned'], data.num_classes)\n",
    "        self.classificationCriterion = nn.CrossEntropyLoss()\n",
    "        self.syntheticCriterion = nn.MSELoss()\n",
    "        self.plot = args['plot']\n",
    "        self.num_epochs = args['num_epochs']\n",
    "        self.model_name = args['model_name']\n",
    "        self.conditioned = args['conditioned']\n",
    "        self.best_perf = 0.\n",
    "        self.stats = dict(grad_loss=[], classify_loss=[])\n",
    "\n",
    "    def optimizer_module(self, optimizer, forward, out, label_onehot=None):\n",
    "        optimizer.zero_grad()\n",
    "        out, grad = forward(out, label_onehot)\n",
    "        out.backward(grad.detach().data)\n",
    "        optimizer.step()\n",
    "        out = out.detach()\n",
    "        return out\n",
    "\n",
    "    def save_grad(self, name):\n",
    "        def hook(grad):\n",
    "            self.backprop_grads[name] = grad\n",
    "        return hook\n",
    "\n",
    "    def optimizer_dni_module(self, images, labels, label_onehot, grad_optimizer, optimizer, forward):\n",
    "        grad_optimizer.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        outs, grads = forward(images, label_onehot)\n",
    "        self.backprop_grads = {}\n",
    "        handles = {}\n",
    "        keys = []\n",
    "        for i, (out, grad) in enumerate(zip(outs, grads)):\n",
    "            handles[str(i)] = out.register_hook(self.save_grad(str(i)))\n",
    "            keys.append(str(i))\n",
    "        outputs = outs[-1]\n",
    "        loss = self.classificationCriterion(outputs, labels)\n",
    "        with torch.no_grad():\n",
    "            loss.backward(retain_graph=True)\n",
    "        for (k, v) in handles.items():\n",
    "            v.remove()\n",
    "        grad_loss = 0.\n",
    "        for k in keys:\n",
    "            grad_loss += self.syntheticCriterion(grads[int(k)], self.backprop_grads[k].detach())\n",
    "\n",
    "        grad_loss.backward()\n",
    "        grad_optimizer.step()\n",
    "        self.stats['grad_loss'].append(grad_loss.item())\n",
    "        self.stats['classify_loss'].append(loss.item())\n",
    "        return loss, grad_loss\n",
    "\n",
    "    def train_model(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for i, (images, labels) in enumerate(self.train_loader):  \n",
    "                labels_onehot = torch.zeros([labels.size(0), self.num_classes])\n",
    "                labels_onehot.scatter_(1, labels.unsqueeze(1), 1)  \n",
    "                labels_onehot = Variable(labels_onehot)\n",
    "                out = images\n",
    "                for (optimizer, forward) in zip(self.net.optimizers, self.net.forwards):\n",
    "                    if self.conditioned:\n",
    "                        out = self.optimizer_module(optimizer, forward, out, labels_onehot)\n",
    "                    else:\n",
    "                        out = self.optimizer_module(optimizer, forward, out)\n",
    "                loss, grad_loss = self.optimizer_dni_module(images, labels, labels_onehot, \n",
    "                                          self.net.grad_optimizer, self.net.optimizer, self.net)\n",
    "                \n",
    "                if (i+1) % 100 == 0:\n",
    "                    print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Grad Loss: %.4f' \n",
    "                         %(epoch+1, self.num_epochs, i+1, self.num_train//self.batch_size, loss.item(), grad_loss.item()))\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                perf = self.test_model(epoch+1)    \n",
    "                if perf > self.best_perf:\n",
    "                    torch.save(self.net.state_dict(), self.model_name+'_model_best.pkl')\n",
    "                    self.net.train()\n",
    "\n",
    "        pkl.dump(self.stats, open(self.model_name+'_stats.pkl', 'wb'))\n",
    "        torch.save(self.net.state_dict(), self.model_name+'_model.pkl')\n",
    "        if self.plot:\n",
    "            plot(self.stats, name=self.model_name)\n",
    "\n",
    "    def test_model(self, epoch):\n",
    "        self.net.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in self.test_loader:\n",
    "            images = Variable(images)\n",
    "            outputs = self.net(images)\n",
    "            outputs = outputs[-1]\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu() == labels).sum()\n",
    "        perf = 100 * correct / total\n",
    "        print('Epoch %d: Accuracy of the network on the 10000 test images: %d %%' % (epoch, perf))\n",
    "        return perf\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset\": 'cifar10',\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 100,\n",
    "    \"conditioned\": False,\n",
    "    \"plot\": True,\n",
    "    \"model_type\": \"cnn\"\n",
    "}\n",
    "\n",
    "model_name = '%s.%s_dni'%(args['dataset'], args['model_type'], )\n",
    "if args['conditioned']:\n",
    "    model_name += '.conditioned'\n",
    "args['model_name'] = model_name\n",
    "if args['dataset'] == 'mnist':\n",
    "    data = mnist(args)\n",
    "elif args['dataset'] == 'cifar10':\n",
    "    data = cifar10(args)\n",
    "    \n",
    "m = classifier(args, data)\n",
    "m.train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f94c6b32fbda5dcd64daf382f382d2d5da78e483f351d87e126340144fcf47d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
