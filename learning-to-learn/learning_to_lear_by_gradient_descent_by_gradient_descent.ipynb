{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Learning to learn by gradient descent by gradient descent\" paper implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import multiprocessing\n",
    "import os.path\n",
    "import csv\n",
    "import copy\n",
    "import joblib\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "from pdb import set_trace as bp\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "def w(v):\n",
    "    if USE_CUDA:\n",
    "        return v.cuda()\n",
    "    return v\n",
    "  \n",
    "cache = joblib.Memory(location='_cache', verbose=0)\n",
    "\n",
    "from meta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def detach_var(v):\n",
    "    var = w(Variable(v.data, requires_grad=True))\n",
    "    var.retain_grad()\n",
    "    return var\n",
    "\n",
    "def rsetattr(obj, attr, val):\n",
    "    pre, _, post = attr.rpartition('.')\n",
    "    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n",
    "\n",
    "def rgetattr(obj, attr, *args):\n",
    "    def _getattr(obj, attr):\n",
    "        return getattr(obj, attr, *args)\n",
    "    return functools.reduce(_getattr, [obj] + attr.split('.'))\n",
    "\n",
    "def do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True):\n",
    "    if should_train:\n",
    "        opt_net.train()\n",
    "    else:\n",
    "        opt_net.eval()\n",
    "        unroll = 1\n",
    "    \n",
    "    target = target_cls(training=should_train)\n",
    "    optimizee = w(target_to_opt())\n",
    "    n_params = 0\n",
    "    for name, p in optimizee.all_named_parameters():\n",
    "        n_params += int(np.prod(p.size()))\n",
    "    hidden_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
    "    cell_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
    "    all_losses_ever = []\n",
    "    if should_train:\n",
    "        meta_opt.zero_grad()\n",
    "    all_losses = None\n",
    "    for iteration in range(1, optim_it + 1):\n",
    "        loss = optimizee(target)\n",
    "                    \n",
    "        if all_losses is None:\n",
    "            all_losses = loss\n",
    "        else:\n",
    "            all_losses += loss\n",
    "        \n",
    "        all_losses_ever.append(loss.data.cpu().numpy())\n",
    "        loss.backward(retain_graph=should_train)\n",
    "\n",
    "        offset = 0\n",
    "        result_params = {}\n",
    "        hidden_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
    "        cell_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
    "        for name, p in optimizee.all_named_parameters():\n",
    "            cur_sz = int(np.prod(p.size()))\n",
    "            gradients = detach_var(p.grad.view(cur_sz, 1))\n",
    "            updates, new_hidden, new_cell = opt_net(\n",
    "                gradients,\n",
    "                [h[offset:offset+cur_sz] for h in hidden_states],\n",
    "                [c[offset:offset+cur_sz] for c in cell_states]\n",
    "            )\n",
    "            for i in range(len(new_hidden)):\n",
    "                hidden_states2[i][offset:offset+cur_sz] = new_hidden[i]\n",
    "                cell_states2[i][offset:offset+cur_sz] = new_cell[i]\n",
    "            result_params[name] = p + updates.view(*p.size()) * out_mul\n",
    "            result_params[name].retain_grad()\n",
    "            \n",
    "            offset += cur_sz\n",
    "            \n",
    "        if iteration % unroll == 0:\n",
    "            if should_train:\n",
    "                meta_opt.zero_grad()\n",
    "                all_losses.backward()\n",
    "                meta_opt.step()\n",
    "                \n",
    "            all_losses = None\n",
    "\n",
    "            optimizee = w(target_to_opt())\n",
    "            optimizee.load_state_dict(result_params)\n",
    "            optimizee.zero_grad()\n",
    "            hidden_states = [detach_var(v) for v in hidden_states2]\n",
    "            cell_states = [detach_var(v) for v in cell_states2]\n",
    "            \n",
    "        else:\n",
    "            for name, p in optimizee.all_named_parameters():\n",
    "                rsetattr(optimizee, name, result_params[name])\n",
    "            assert len(list(optimizee.all_named_parameters()))\n",
    "            hidden_states = hidden_states2\n",
    "            cell_states = cell_states2\n",
    "            \n",
    "    return all_losses_ever\n",
    "\n",
    "class Optimizer(nn.Module):\n",
    "    def __init__(self, preproc=False, hidden_sz=20, preproc_factor=10.0):\n",
    "        super().__init__()\n",
    "        self.hidden_sz = hidden_sz\n",
    "        if preproc:\n",
    "            self.recurs = nn.LSTMCell(2, hidden_sz)\n",
    "        else:\n",
    "            self.recurs = nn.LSTMCell(1, hidden_sz)\n",
    "        self.recurs2 = nn.LSTMCell(hidden_sz, hidden_sz)\n",
    "        self.output = nn.Linear(hidden_sz, 1)\n",
    "        self.preproc = preproc\n",
    "        self.preproc_factor = preproc_factor\n",
    "        self.preproc_threshold = np.exp(-preproc_factor)\n",
    "\n",
    "    def forward(self, inp, hidden, cell):\n",
    "        if self.preproc:\n",
    "            inp = inp.data\n",
    "            inp2 = w(torch.zeros(inp.size()[0], 2))\n",
    "            keep_grads = (torch.abs(inp) >= self.preproc_threshold).squeeze()\n",
    "            inp2[:, 0][keep_grads] = (\n",
    "                torch.log(torch.abs(inp[keep_grads]) + 1e-8) / self.preproc_factor).squeeze()\n",
    "            inp2[:, 1][keep_grads] = torch.sign(inp[keep_grads]).squeeze()\n",
    "\n",
    "            inp2[:, 0][~keep_grads] = -1\n",
    "            inp2[:, 1][~keep_grads] = (\n",
    "                float(np.exp(self.preproc_factor)) * inp[~keep_grads]).squeeze()\n",
    "            inp = w(Variable(inp2))\n",
    "        hidden0, cell0 = self.recurs(inp, (hidden[0], cell[0]))\n",
    "        hidden1, cell1 = self.recurs2(hidden0, (hidden[1], cell[1]))\n",
    "        return self.output(hidden1), (hidden0, hidden1), (cell0, cell1)\n",
    "\n",
    "@cache.cache\n",
    "def fit_optimizer(target_cls, target_to_opt, preproc=False, unroll=20, optim_it=100, n_epochs=20, n_tests=100, lr=0.001, out_mul=1.0):\n",
    "    opt_net = w(Optimizer(preproc=preproc))\n",
    "    meta_opt = optim.Adam(opt_net.parameters(), lr=lr)\n",
    "    \n",
    "    best_net = None\n",
    "    best_loss = 100000000000000000\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "        for _ in range(20):\n",
    "            do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True)\n",
    "        \n",
    "        loss = (np.mean([\n",
    "            np.sum(do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=False))\n",
    "            for _ in range(n_tests)\n",
    "        ]))\n",
    "        print(loss)\n",
    "        if loss < best_loss:\n",
    "            print(best_loss, loss)\n",
    "            best_loss = loss\n",
    "            best_net = copy.deepcopy(opt_net.state_dict())\n",
    "            \n",
    "    return best_loss, best_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticLoss:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.W = w(Variable(torch.randn(10, 10)))\n",
    "        self.y = w(Variable(torch.randn(10)))\n",
    "\n",
    "    def get_loss(self, theta):\n",
    "        return torch.sum((self.W.matmul(theta) - self.y)**2)\n",
    "\n",
    "\n",
    "class QuadOptimizee(MetaModule):\n",
    "    def __init__(self, theta=None):\n",
    "        super().__init__()\n",
    "        self.register_buffer('theta', to_var(\n",
    "            torch.zeros(10), requires_grad=True))\n",
    "\n",
    "    def forward(self, target):\n",
    "        return target.get_loss(self.theta)\n",
    "\n",
    "    def all_named_parameters(self):\n",
    "        return [('theta', self.theta)]\n",
    "\n",
    "for lr in [1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001]:\n",
    "    print('Trying learning rate:', lr)\n",
    "    print(fit_optimizer(QuadraticLoss, QuadOptimizee, lr=lr)[0])\n",
    "\n",
    "loss, quad_optimizer = fit_optimizer(\n",
    "    QuadraticLoss, QuadOptimizee, lr=0.003, n_epochs=100)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache.cache\n",
    "def fit_normal(target_cls, target_to_opt, opt_class, n_tests=100, n_epochs=100, **kwargs):\n",
    "    results = []\n",
    "    for i in range(n_tests):\n",
    "        target = target_cls(training=False)\n",
    "        optimizee = w(target_to_opt())\n",
    "        optimizer = opt_class(optimizee.parameters(), **kwargs)\n",
    "        total_loss = []\n",
    "        for _ in range(n_epochs):\n",
    "            loss = optimizee(target)\n",
    "            \n",
    "            total_loss.append(loss.data.cpu().numpy())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        results.append(total_loss)\n",
    "    return results\n",
    "\n",
    "def find_best_lr_normal(target_cls, target_to_opt, opt_class, **extra_kwargs):\n",
    "    best_loss = 1000000000000000.0\n",
    "    best_lr = 0.0\n",
    "    for lr in [1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001]:\n",
    "        try:\n",
    "            loss = best_loss + 1.0\n",
    "            loss = np.mean([np.sum(s) for s in fit_normal(target_cls, target_to_opt, opt_class, lr=lr, **extra_kwargs)])\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_lr = lr\n",
    "    return best_loss, \n",
    "  \n",
    "@cache.cache\n",
    "def fit_normal(target_cls, target_to_opt, opt_class, n_tests=100, n_epochs=100, **kwargs):\n",
    "    results = []\n",
    "    for i in tqdm(range(n_tests), 'tests'):\n",
    "        target = target_cls(training=False)\n",
    "        optimizee = w(target_to_opt())\n",
    "        optimizer = opt_class(optimizee.parameters(), **kwargs)\n",
    "        total_loss = []\n",
    "        for _ in range(n_epochs):\n",
    "            loss = optimizee(target)\n",
    "            \n",
    "            total_loss.append(loss.data.cpu().numpy())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        results.append(total_loss)\n",
    "    return results\n",
    "\n",
    "def find_best_lr_normal(target_cls, target_to_opt, opt_class, **extra_kwargs):\n",
    "    best_loss = 1000000000000000.0\n",
    "    best_lr = 0.0\n",
    "    for lr in [1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001]:\n",
    "        try:\n",
    "            loss = best_loss + 1.0\n",
    "            loss = np.mean([np.sum(s) for s in fit_normal(target_cls, target_to_opt, opt_class, lr=lr, **extra_kwargs)])\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_lr = lr\n",
    "    return best_loss\n",
    "  \n",
    "NORMAL_OPTS = [(optim.Adam, {}), (optim.RMSprop, {}), (optim.SGD, {'momentum': 0.9}), (optim.SGD, {'nesterov': True, 'momentum': 0.9})]\n",
    "OPT_NAMES = ['ADAM', 'RMSprop', 'SGD', 'NAG']\n",
    "\n",
    "for opt, kwargs in NORMAL_OPTS:\n",
    "    print(find_best_lr_normal(QuadraticLoss, QuadOptimizee, opt, **kwargs))\n",
    "    \n",
    "QUAD_LRS = [0.1, 0.03, 0.01, 0.01]\n",
    "fit_data = np.zeros((100, 100, len(OPT_NAMES) + 1))\n",
    "for i, ((opt, extra_kwargs), lr) in enumerate(zip(NORMAL_OPTS, QUAD_LRS)):\n",
    "    np.random.seed(0)\n",
    "    fit_data[:, :, i] = np.array(fit_normal(QuadraticLoss, QuadOptimizee, opt, lr=lr, **extra_kwargs))\n",
    "\n",
    "opt = w(Optimizer())\n",
    "opt.load_state_dict(quad_optimizer)\n",
    "np.random.seed(0)\n",
    "fit_data[:, :, len(OPT_NAMES)] = np.array([do_fit(opt, None, QuadraticLoss, QuadOptimizee, 1, 100, 100, out_mul=1.0, should_train=False) for _ in range(100)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f94c6b32fbda5dcd64daf382f382d2d5da78e483f351d87e126340144fcf47d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
