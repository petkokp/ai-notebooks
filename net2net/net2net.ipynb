{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Net2Net: ACCELERATING LEARNING VIA KNOWLEDGE TRANSFER\" paper implementation - https://arxiv.org/pdf/1511.05641.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def net_to_wider_net(m1, m2, new_width, bnorm=None, out_size=None, noise=True,\n",
    "          random_init=True, weight_norm=True):\n",
    "    w1 = m1.weight.data\n",
    "    w2 = m2.weight.data\n",
    "    b1 = m1.bias.data\n",
    "\n",
    "    if \"Conv\" in m1.__class__.__name__ or \"Linear\" in m1.__class__.__name__:\n",
    "        if \"Conv\" in m1.__class__.__name__ and \"Linear\" in m2.__class__.__name__:\n",
    "            assert w2.size(1) % w1.size(\n",
    "                0) == 0, \"Linear units need to be multiple\"\n",
    "            if w1.dim() == 4:\n",
    "                factor = int(np.sqrt(w2.size(1) // w1.size(0)))\n",
    "                w2 = w2.view(w2.size(0), w2.size(1)//factor**2, factor, factor)\n",
    "            elif w1.dim() == 5:\n",
    "                assert out_size is not None, \"For conv3d -> linear out_size is necessary\"\n",
    "                factor = out_size[0] * out_size[1] * out_size[2]\n",
    "                w2 = w2.view(w2.size(0), w2.size(1)//factor, out_size[0],\n",
    "                             out_size[1], out_size[2])\n",
    "\n",
    "        old_width = w1.size(0)\n",
    "        nw1 = m1.weight.data.clone()\n",
    "        nw2 = w2.clone()\n",
    "\n",
    "        if nw1.dim() == 4:\n",
    "            nw1.resize_(new_width, nw1.size(1), nw1.size(2), nw1.size(3))\n",
    "            nw2.resize_(nw2.size(0), new_width, nw2.size(2), nw2.size(3))\n",
    "        elif nw1.dim() == 5:\n",
    "            nw1.resize_(new_width, nw1.size(1), nw1.size(2),\n",
    "                        nw1.size(3), nw1.size(4))\n",
    "            nw2.resize_(nw2.size(0), new_width, nw2.size(2),\n",
    "                        nw2.size(3), nw2.size(4))\n",
    "        else:\n",
    "            nw1.resize_(new_width, nw1.size(1))\n",
    "            nw2.resize_(nw2.size(0), new_width)\n",
    "\n",
    "        if b1 is not None:\n",
    "            nb1 = m1.bias.data.clone()\n",
    "            nb1.resize_(new_width)\n",
    "\n",
    "        if bnorm is not None:\n",
    "            nrunning_mean = bnorm.running_mean.clone().resize_(new_width)\n",
    "            nrunning_var = bnorm.running_var.clone().resize_(new_width)\n",
    "            if bnorm.affine:\n",
    "                nweight = bnorm.weight.data.clone().resize_(new_width)\n",
    "                nbias = bnorm.bias.data.clone().resize_(new_width)\n",
    "\n",
    "        w2 = w2.transpose(0, 1)\n",
    "        nw2 = nw2.transpose(0, 1)\n",
    "\n",
    "        nw1.narrow(0, 0, old_width).copy_(w1)\n",
    "        nw2.narrow(0, 0, old_width).copy_(w2)\n",
    "        nb1.narrow(0, 0, old_width).copy_(b1)\n",
    "\n",
    "        if bnorm is not None:\n",
    "            nrunning_var.narrow(0, 0, old_width).copy_(bnorm.running_var)\n",
    "            nrunning_mean.narrow(0, 0, old_width).copy_(bnorm.running_mean)\n",
    "            if bnorm.affine:\n",
    "                nweight.narrow(0, 0, old_width).copy_(bnorm.weight.data)\n",
    "                nbias.narrow(0, 0, old_width).copy_(bnorm.bias.data)\n",
    "\n",
    "        if weight_norm:\n",
    "            for i in range(old_width):\n",
    "                norm = w1.select(0, i).norm()\n",
    "                w1.select(0, i).div_(norm)\n",
    "\n",
    "        tracking = dict()\n",
    "        for i in range(old_width, new_width):\n",
    "            idx = np.random.randint(0, old_width)\n",
    "            try:\n",
    "                tracking[idx].append(i)\n",
    "            except:\n",
    "                tracking[idx] = [idx]\n",
    "                tracking[idx].append(i)\n",
    "\n",
    "            if random_init:\n",
    "                n = m1.kernel_size[0] * m1.kernel_size[1] * m1.out_channels\n",
    "                if m2.weight.dim() == 4:\n",
    "                    n2 = m2.kernel_size[0] * \\\n",
    "                        m2.kernel_size[1] * m2.out_channels\n",
    "                elif m2.weight.dim() == 5:\n",
    "                    n2 = m2.kernel_size[0] * m2.kernel_size[1] * \\\n",
    "                        m2.kernel_size[2] * m2.out_channels\n",
    "                elif m2.weight.dim() == 2:\n",
    "                    n2 = m2.out_features * m2.in_features\n",
    "                nw1.select(0, i).normal_(0, np.sqrt(2./n))\n",
    "                nw2.select(0, i).normal_(0, np.sqrt(2./n2))\n",
    "            else:\n",
    "                nw1.select(0, i).copy_(w1.select(0, idx).clone())\n",
    "                nw2.select(0, i).copy_(w2.select(0, idx).clone())\n",
    "            nb1[i] = b1[idx]\n",
    "\n",
    "        if bnorm is not None:\n",
    "            nrunning_mean[i] = bnorm.running_mean[idx]\n",
    "            nrunning_var[i] = bnorm.running_var[idx]\n",
    "            if bnorm.affine:\n",
    "                nweight[i] = bnorm.weight.data[idx]\n",
    "                nbias[i] = bnorm.bias.data[idx]\n",
    "            bnorm.num_features = new_width\n",
    "\n",
    "        if not random_init:\n",
    "            for idx, d in tracking.items():\n",
    "                for item in d:\n",
    "                    nw2[item].div_(len(d))\n",
    "\n",
    "        w2.transpose_(0, 1)\n",
    "        nw2.transpose_(0, 1)\n",
    "\n",
    "        m1.out_channels = new_width\n",
    "        m2.in_channels = new_width\n",
    "\n",
    "        if noise:\n",
    "            noise = np.random.normal(scale=5e-2 * nw1.std(),\n",
    "                                     size=list(nw1.size()))\n",
    "            nw1 += th.FloatTensor(noise).type_as(nw1)\n",
    "\n",
    "        m1.weight.data = nw1\n",
    "\n",
    "        if \"Conv\" in m1.__class__.__name__ and \"Linear\" in m2.__class__.__name__:\n",
    "            if w1.dim() == 4:\n",
    "                m2.weight.data = nw2.view(\n",
    "                    m2.weight.size(0), new_width*factor**2)\n",
    "                m2.in_features = new_width*factor**2\n",
    "            elif w2.dim() == 5:\n",
    "                m2.weight.data = nw2.view(m2.weight.size(0), new_width*factor)\n",
    "                m2.in_features = new_width*factor\n",
    "        else:\n",
    "            m2.weight.data = nw2\n",
    "\n",
    "        m1.bias.data = nb1\n",
    "\n",
    "        if bnorm is not None:\n",
    "            bnorm.running_var = nrunning_var\n",
    "            bnorm.running_mean = nrunning_mean\n",
    "            if bnorm.affine:\n",
    "                bnorm.weight.data = nweight\n",
    "                bnorm.bias.data = nbias\n",
    "\n",
    "        return m1, m2, bnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_to_deeper_net(m, nonlin, bnorm_flag=False, weight_norm=True, noise=True):\n",
    "    if \"Linear\" in m.__class__.__name__:\n",
    "        m2 = th.nn.Linear(m.out_features, m.out_features)\n",
    "        m2.weight.data.copy_(th.eye(m.out_features))\n",
    "        m2.bias.data.zero_()\n",
    "\n",
    "        if bnorm_flag:\n",
    "            bnorm = th.nn.BatchNorm1d(m2.weight.size(1))\n",
    "            bnorm.weight.data.fill_(1)\n",
    "            bnorm.bias.data.fill_(0)\n",
    "            bnorm.running_mean.fill_(0)\n",
    "            bnorm.running_var.fill_(1)\n",
    "\n",
    "    elif \"Conv\" in m.__class__.__name__:\n",
    "        assert m.kernel_size[0] % 2 == 1, \"Kernel size needs to be odd\"\n",
    "\n",
    "        if m.weight.dim() == 4:\n",
    "            pad_h = int((m.kernel_size[0] - 1) / 2)\n",
    "            m2 = th.nn.Conv2d(m.out_channels, m.out_channels,\n",
    "                              kernel_size=m.kernel_size, padding=pad_h)\n",
    "            m2.weight.data.zero_()\n",
    "            c = m.kernel_size[0] // 2 + 1\n",
    "\n",
    "        elif m.weight.dim() == 5:\n",
    "            pad_hw = int((m.kernel_size[1] - 1) / 2)\n",
    "            pad_d = int((m.kernel_size[0] - 1) / 2)\n",
    "            m2 = th.nn.Conv3d(m.out_channels,\n",
    "                              m.out_channels,\n",
    "                              kernel_size=m.kernel_size,\n",
    "                              padding=(pad_d, pad_hw, pad_hw))\n",
    "            c_wh = m.kernel_size[1] // 2 + 1\n",
    "            c_d = m.kernel_size[0] // 2 + 1\n",
    "\n",
    "        restore = False\n",
    "        if m2.weight.dim() == 2:\n",
    "            restore = True\n",
    "            m2.weight.data = m2.weight.data.view(m2.weight.size(0),\n",
    "                                                 m2.in_channels,\n",
    "                                                 m2.kernel_size[0],\n",
    "                                                 m2.kernel_size[0])\n",
    "\n",
    "        if weight_norm:\n",
    "            for i in range(m.out_channels):\n",
    "                weight = m.weight.data\n",
    "                norm = weight.select(0, i).norm()\n",
    "                weight.div_(norm)\n",
    "                m.weight.data = weight\n",
    "\n",
    "        for i in range(0, m.out_channels):\n",
    "            if m.weight.dim() == 4:\n",
    "                m2.weight.data.narrow(0, i, 1).narrow(\n",
    "                    1, i, 1).narrow(2, c, 1).narrow(3, c, 1).fill_(1)\n",
    "            elif m.weight.dim() == 5:\n",
    "                m2.weight.data.narrow(0, i, 1).narrow(1, i, 1).narrow(\n",
    "                    2, c_d, 1).narrow(3, c_wh, 1).narrow(4, c_wh, 1).fill_(1)\n",
    "\n",
    "        if noise:\n",
    "            noise = np.random.normal(scale=5e-2 * m2.weight.data.std(),\n",
    "                                     size=list(m2.weight.size()))\n",
    "            m2.weight.data += th.FloatTensor(noise).type_as(m2.weight.data)\n",
    "\n",
    "        if restore:\n",
    "            m2.weight.data = m2.weight.data.view(m2.weight.size(0),\n",
    "                                                 m2.in_channels,\n",
    "                                                 m2.kernel_size[0],\n",
    "                                                 m2.kernel_size[0])\n",
    "\n",
    "        m2.bias.data.zero_()\n",
    "\n",
    "        if bnorm_flag:\n",
    "            if m.weight.dim() == 4:\n",
    "                bnorm = th.nn.BatchNorm2d(m2.out_channels)\n",
    "            elif m.weight.dim() == 5:\n",
    "                bnorm = th.nn.BatchNorm3d(m2.out_channels)\n",
    "            bnorm.weight.data.fill_(1)\n",
    "            bnorm.bias.data.fill_(0)\n",
    "            bnorm.running_mean.fill_(0)\n",
    "            bnorm.running_var.fill_(1)\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"{} Module not supported\".format(m.__class__.__name__))\n",
    "\n",
    "    s = th.nn.Sequential()\n",
    "    s.add_module('conv', m)\n",
    "    if bnorm_flag:\n",
    "        s.add_module('bnorm', bnorm)\n",
    "    if nonlin is not None:\n",
    "        s.add_module('nonlin', nonlin())\n",
    "    s.add_module('conv_new', m2)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "seed = 1\n",
    "log_interval = 100\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.size(1)*x.size(2)*x.size(3))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "    def net2net_wider(self):\n",
    "        self.conv1, self.conv2, _ = net_to_wider_net(self.conv1, self.conv2, 15)\n",
    "        self.conv2, self.fc1, _ = net_to_wider_net(self.conv2, self.fc1, 30)\n",
    "        print(self)\n",
    "\n",
    "    def net2net_deeper(self):\n",
    "        s = net_to_deeper_net(self.conv1, nn.ReLU, bnorm_flag=False)\n",
    "        self.conv1 = s\n",
    "        s = net_to_deeper_net(self.conv2, nn.ReLU, bnorm_flag=False)\n",
    "        self.conv2 = s\n",
    "        print(self)\n",
    "\n",
    "    def define_wider(self):\n",
    "        self.conv1 = nn.Conv2d(1, 15, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(15, 30, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(480, 50)\n",
    "\n",
    "    def define_wider_deeper(self):\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, 15, kernel_size=5),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv2d(15, 15, kernel_size=5, padding=2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(15, 30, kernel_size=5),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv2d(30, 30, kernel_size=5, padding=2))\n",
    "        self.fc1 = nn.Linear(480, 50)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "        return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " > Teacher training... \n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.371851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1166/2283106900.py:52: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.066881\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.206788\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.744905\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.599226\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.800742\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.752787\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.515826\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.474530\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.466711\n",
      "\n",
      "Test set: Average loss: 0.2099, Accuracy: 9407/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.369939\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.456399\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.384596\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.451844\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.402165\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.289913\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.308951\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.618315\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.340878\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.297404\n",
      "\n",
      "Test set: Average loss: 0.1255, Accuracy: 9614/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.464201\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.437607\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.292086\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.405149\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.310390\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.202899\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.456441\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.435543\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.319803\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.378708\n",
      "\n",
      "Test set: Average loss: 0.1013, Accuracy: 9692/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.202678\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.521868\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.268599\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.278162\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.297523\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.066652\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.340492\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.135988\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.265102\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.175810\n",
      "\n",
      "Test set: Average loss: 0.0847, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.127273\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.271650\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.278630\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.241822\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.185338\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.286777\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.126861\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.263046\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.197175\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.235591\n",
      "\n",
      "Test set: Average loss: 0.0740, Accuracy: 9762/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.165885\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.117183\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.185652\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.209297\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.175537\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.199415\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.174854\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.128522\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.155142\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.320528\n",
      "\n",
      "Test set: Average loss: 0.0689, Accuracy: 9772/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.299323\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.158874\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.236853\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.034023\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.150303\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.253249\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.315158\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.261026\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.120729\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.296437\n",
      "\n",
      "Test set: Average loss: 0.0635, Accuracy: 9799/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.146195\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.161812\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.295173\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.267615\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.160370\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.149521\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.045294\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.276091\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.132548\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.088658\n",
      "\n",
      "Test set: Average loss: 0.0599, Accuracy: 9812/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.148441\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.148833\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.201268\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.225427\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.096212\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.332621\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.405567\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.088200\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.176738\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.110569\n",
      "\n",
      "Test set: Average loss: 0.0539, Accuracy: 9831/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.181005\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.154848\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.166377\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.089480\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.394325\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.190859\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.140504\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.187585\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.253437\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.201953\n",
      "\n",
      "Test set: Average loss: 0.0524, Accuracy: 9845/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n > Teacher training... \")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    teacher_accu = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " > Wider Student training... \n",
      "Net(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(15, 30, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=480, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.387631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1166/2545197502.py:52: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.621406\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.823580\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.691162\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.574728\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.593143\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.420739\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.445403\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.487250\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.364731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petko/projects/ai-notebooks/env/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1749, Accuracy: 9476/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.422646\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.199908\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.270411\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.560420\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.458246\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.345032\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.355656\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.254552\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.434722\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.304286\n",
      "\n",
      "Test set: Average loss: 0.1047, Accuracy: 9676/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.161064\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.091402\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.286445\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.275425\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.214435\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.115487\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.322100\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.367638\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.212833\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.324971\n",
      "\n",
      "Test set: Average loss: 0.0874, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.269366\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.206032\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.323539\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.230991\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.251992\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.128617\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.202736\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.385559\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.174609\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.258903\n",
      "\n",
      "Test set: Average loss: 0.0746, Accuracy: 9763/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.128495\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.212884\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.224172\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.132856\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.060473\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.230234\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.194481\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.234786\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.192183\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.161217\n",
      "\n",
      "Test set: Average loss: 0.0687, Accuracy: 9787/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.206948\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.135579\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.112586\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.196801\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.423377\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.141165\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.231219\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.128151\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.060782\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.227867\n",
      "\n",
      "Test set: Average loss: 0.0635, Accuracy: 9816/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.172865\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.123581\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.202588\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.162004\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.039339\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.265618\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.139429\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.144633\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.167426\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.237851\n",
      "\n",
      "Test set: Average loss: 0.0565, Accuracy: 9827/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.211117\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.110414\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.219192\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.252765\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.071923\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.065474\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.082281\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.126098\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.191232\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.345652\n",
      "\n",
      "Test set: Average loss: 0.0527, Accuracy: 9846/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.172847\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.309217\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.172017\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.122906\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.201144\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.138495\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.093208\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.093110\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.094424\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.098685\n",
      "\n",
      "Test set: Average loss: 0.0482, Accuracy: 9845/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.271384\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.067469\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.145162\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.237291\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.129492\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.196616\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.346581\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.267238\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.193743\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.296697\n",
      "\n",
      "Test set: Average loss: 0.0457, Accuracy: 9848/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n > Wider Student training... \")\n",
    "model_ = Net()\n",
    "model_ = copy.deepcopy(model)\n",
    "\n",
    "del model\n",
    "model = model_\n",
    "model.net2net_wider()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    wider_accu = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " > Wider + Deeper Student training... \n",
      "Net(\n",
      "  (conv1): Sequential(\n",
      "    (conv): Conv2d(1, 15, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (nonlin): ReLU()\n",
      "    (conv_new): Conv2d(15, 15, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (conv): Conv2d(15, 30, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (nonlin): ReLU()\n",
      "    (conv_new): Conv2d(30, 30, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=480, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.048279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1166/2545197502.py:52: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.184439\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.259213\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.150717\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.067047\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.214770\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.321404\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.140248\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.115955\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.203985\n",
      "\n",
      "Test set: Average loss: 0.0523, Accuracy: 9841/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.140832\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.273180\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.105051\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.048267\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.091193\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.218311\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.092276\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.077792\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.174223\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.114439\n",
      "\n",
      "Test set: Average loss: 0.0388, Accuracy: 9879/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.110652\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.066847\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.067909\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.247053\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.092122\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.128741\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.079254\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.203414\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.157128\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.044943\n",
      "\n",
      "Test set: Average loss: 0.0333, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.181204\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.243092\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.144556\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.288661\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.122131\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.111613\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.054162\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.136525\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.049455\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.092754\n",
      "\n",
      "Test set: Average loss: 0.0348, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.061689\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.041602\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.083296\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.102771\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.056453\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.087326\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.078877\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.025922\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.067001\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.031368\n",
      "\n",
      "Test set: Average loss: 0.0285, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.241884\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.086237\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.078859\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.308745\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.022281\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.043476\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.037138\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.015549\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.020398\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.020191\n",
      "\n",
      "Test set: Average loss: 0.0262, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.021016\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.099590\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.052342\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.010868\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.105854\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.022748\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.119826\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.045146\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.044202\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.018226\n",
      "\n",
      "Test set: Average loss: 0.0278, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.023227\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.068302\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.045839\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.049879\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.054064\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.020049\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.150335\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.101134\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.133749\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.038679\n",
      "\n",
      "Test set: Average loss: 0.0283, Accuracy: 9920/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.067945\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.099210\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.041825\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.119226\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.023175\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.105643\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.024179\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.184239\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.034833\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.069779\n",
      "\n",
      "Test set: Average loss: 0.0252, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.033237\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.027613\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.173248\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.149626\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.211358\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.026020\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.162585\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.045795\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.174594\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.011905\n",
      "\n",
      "Test set: Average loss: 0.0277, Accuracy: 9913/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n > Wider + Deeper Student training... \")\n",
    "model_ = Net()\n",
    "model_ = copy.deepcopy(model)\n",
    "\n",
    "del model\n",
    "model = model_\n",
    "model.net2net_deeper()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    deeper_accu = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " > Wider teacher training... \n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.323987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1166/2545197502.py:52: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.883135\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.049915\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.857313\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.531759\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.570595\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.533823\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.395554\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.504689\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.499775\n",
      "\n",
      "Test set: Average loss: 0.1597, Accuracy: 9524/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.439629\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.228204\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.381115\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.218179\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.246535\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.282480\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.352200\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.278183\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.391891\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.357527\n",
      "\n",
      "Test set: Average loss: 0.1007, Accuracy: 9687/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.334363\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.159145\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.265828\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.220050\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.179674\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.214072\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.268664\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.156169\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.310518\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.151512\n",
      "\n",
      "Test set: Average loss: 0.0822, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.073211\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.242119\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.115281\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.307494\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.217193\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.289445\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.078933\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.097802\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.222946\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.221539\n",
      "\n",
      "Test set: Average loss: 0.0663, Accuracy: 9797/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.193419\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.091909\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.141474\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.199580\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.086230\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.379258\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.241717\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.170282\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.130797\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.312385\n",
      "\n",
      "Test set: Average loss: 0.0581, Accuracy: 9825/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.297683\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.100570\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.269581\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.111811\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.241186\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.286159\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.295048\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.140399\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.077651\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.174824\n",
      "\n",
      "Test set: Average loss: 0.0542, Accuracy: 9832/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.248296\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.161232\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.194640\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.201136\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.063712\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.031387\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.176229\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.175815\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.149388\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.089189\n",
      "\n",
      "Test set: Average loss: 0.0499, Accuracy: 9843/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.295099\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.069725\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.085863\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.265574\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.127244\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.282593\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.181838\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.110733\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.163173\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.129273\n",
      "\n",
      "Test set: Average loss: 0.0468, Accuracy: 9846/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.095947\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.332497\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.079730\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.128382\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.101962\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.022146\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.081275\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.197673\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.108256\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.181002\n",
      "\n",
      "Test set: Average loss: 0.0456, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.136006\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.032705\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.118498\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.079791\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.234857\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.125101\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.049097\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.094918\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.148503\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.141576\n",
      "\n",
      "Test set: Average loss: 0.0421, Accuracy: 9862/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.092371\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.204643\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.151298\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.129041\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.148141\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.150721\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.108933\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.065330\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.149951\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.202428\n",
      "\n",
      "Test set: Average loss: 0.0419, Accuracy: 9864/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.166166\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.064280\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.171322\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.097281\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.051757\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.177789\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.128899\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.047296\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.128437\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.181357\n",
      "\n",
      "Test set: Average loss: 0.0390, Accuracy: 9878/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.157919\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.118353\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.191772\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.041787\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.178428\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.061339\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.293745\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.096892\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.254855\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.205320\n",
      "\n",
      "Test set: Average loss: 0.0382, Accuracy: 9872/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.062693\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.155651\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.109259\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.082785\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.135853\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.094403\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.048723\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.132963\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.044843\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.103239\n",
      "\n",
      "Test set: Average loss: 0.0357, Accuracy: 9878/10000 (99%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.101804\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.177505\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.100114\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.418398\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.081750\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.066031\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.219564\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.070618\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.065302\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.076942\n",
      "\n",
      "Test set: Average loss: 0.0343, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.146783\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.038922\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.150467\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.121100\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.102338\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.092755\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.056695\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.115568\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.109510\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.190902\n",
      "\n",
      "Test set: Average loss: 0.0340, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.218219\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.088011\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.063838\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.117007\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.088393\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.190944\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.175431\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.075040\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.059170\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.182846\n",
      "\n",
      "Test set: Average loss: 0.0333, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.073121\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.179739\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.184431\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.124248\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.058124\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.170093\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.051362\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.050126\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.103856\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.083726\n",
      "\n",
      "Test set: Average loss: 0.0355, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.148420\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.193644\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.138737\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.018283\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.120461\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.245177\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.163314\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.038356\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.125762\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.052041\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.109585\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 0.116865\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.052903\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.110869\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.100577\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.081304\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.098961\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 0.099188\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.109375\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.114347\n",
      "\n",
      "Test set: Average loss: 0.0302, Accuracy: 9903/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n > Wider teacher training... \")\n",
    "model_ = Net()\n",
    "\n",
    "del model\n",
    "model = model_\n",
    "model.define_wider()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "for epoch in range(1, 2*(epochs) + 1):\n",
    "    train(epoch)\n",
    "    wider_teacher_accu = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " > Wider + Deeper teacher training... \n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1166/2545197502.py:52: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.285769\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.187154\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.188742\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.894444\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.479578\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.529480\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.424298\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.396830\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.349261\n",
      "\n",
      "Test set: Average loss: 0.1603, Accuracy: 9528/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.319840\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.224067\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.372141\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.240714\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.224763\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.325720\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.345110\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.345676\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.287336\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.337362\n",
      "\n",
      "Test set: Average loss: 0.0825, Accuracy: 9751/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.582713\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.171235\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.149875\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.366137\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.230640\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.217199\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.066032\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.080362\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.157362\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.059194\n",
      "\n",
      "Test set: Average loss: 0.0688, Accuracy: 9774/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.119280\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.085865\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.161386\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.306723\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.055812\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.077278\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.146815\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.140905\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.095813\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.361519\n",
      "\n",
      "Test set: Average loss: 0.0482, Accuracy: 9846/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.146112\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.044896\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.233872\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.193986\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.141524\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.496534\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.055856\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.214529\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.130240\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.087556\n",
      "\n",
      "Test set: Average loss: 0.0370, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.053371\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.127833\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.195423\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.042838\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.067823\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.025091\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.077507\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.065763\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.055056\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.289075\n",
      "\n",
      "Test set: Average loss: 0.0365, Accuracy: 9881/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.154109\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.099855\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.054362\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.067223\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.166049\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.120864\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.019080\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.024705\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.105756\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.031509\n",
      "\n",
      "Test set: Average loss: 0.0323, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.083463\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.144816\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.345165\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.019808\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.039265\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.058601\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.042662\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.130070\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.150165\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.140773\n",
      "\n",
      "Test set: Average loss: 0.0288, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.007657\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.069776\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.053110\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.132601\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.127178\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.112273\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.039172\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.119064\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.053950\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.111970\n",
      "\n",
      "Test set: Average loss: 0.0302, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.044873\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.012402\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.034704\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.107880\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.204713\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.071069\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.054614\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.017161\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.023160\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.179238\n",
      "\n",
      "Test set: Average loss: 0.0320, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.221115\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.183370\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.027327\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.125619\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.017708\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.041637\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.010640\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.024975\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.113370\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.069814\n",
      "\n",
      "Test set: Average loss: 0.0242, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.124087\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.042180\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.029800\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.010924\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.007173\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.049891\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.020618\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.083204\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.065130\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.091055\n",
      "\n",
      "Test set: Average loss: 0.0263, Accuracy: 9931/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.067935\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.021049\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.012260\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.046272\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.119809\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.192401\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.040694\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.031059\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.020718\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.131281\n",
      "\n",
      "Test set: Average loss: 0.0216, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.066838\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.020347\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.045918\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.204922\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.168836\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.039416\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.033809\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.038004\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.135692\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.011156\n",
      "\n",
      "Test set: Average loss: 0.0239, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.042339\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.022518\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.050234\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.111561\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.005440\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.042886\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.103198\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.038476\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.031440\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.062254\n",
      "\n",
      "Test set: Average loss: 0.0225, Accuracy: 9920/10000 (99%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.031996\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.038335\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.041417\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.104094\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.117937\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.026582\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.108460\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.081659\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.073576\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.006136\n",
      "\n",
      "Test set: Average loss: 0.0229, Accuracy: 9929/10000 (99%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.099585\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.022814\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.097438\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.058079\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.060380\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.059550\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.058599\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.044423\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.009062\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.023894\n",
      "\n",
      "Test set: Average loss: 0.0220, Accuracy: 9925/10000 (99%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.060733\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.000499\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.014205\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.086037\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.003326\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.098796\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.022838\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.006887\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.048292\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.016374\n",
      "\n",
      "Test set: Average loss: 0.0212, Accuracy: 9934/10000 (99%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.067537\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.012747\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.020717\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.022020\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.113565\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.072520\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.051697\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.020990\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.020257\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.014999\n",
      "\n",
      "Test set: Average loss: 0.0208, Accuracy: 9938/10000 (99%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.013508\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 0.017005\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.011504\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.065720\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.022946\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.017693\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.048733\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 0.013997\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.009842\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.043399\n",
      "\n",
      "Test set: Average loss: 0.0204, Accuracy: 9933/10000 (99%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.040202\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 0.077199\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.027182\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 0.071348\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.034816\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 0.041367\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.016640\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 0.242689\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.006385\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 0.026675\n",
      "\n",
      "Test set: Average loss: 0.0184, Accuracy: 9948/10000 (99%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.049687\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 0.077802\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.042512\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 0.298371\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.049898\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 0.033853\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.041048\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 0.018374\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.043991\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 0.005228\n",
      "\n",
      "Test set: Average loss: 0.0205, Accuracy: 9939/10000 (99%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.015946\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 0.015864\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.168622\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 0.014704\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.055563\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 0.083592\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.013238\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 0.183114\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.449273\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 0.012065\n",
      "\n",
      "Test set: Average loss: 0.0180, Accuracy: 9942/10000 (99%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.058087\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 0.052069\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.024741\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 0.022100\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.075475\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 0.009508\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.035303\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 0.014026\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.016000\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 0.006458\n",
      "\n",
      "Test set: Average loss: 0.0201, Accuracy: 9940/10000 (99%)\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.066344\n",
      "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 0.036482\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.083733\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 0.205314\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.014153\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 0.043634\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.118248\n",
      "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 0.016273\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.042121\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 0.007615\n",
      "\n",
      "Test set: Average loss: 0.0205, Accuracy: 9942/10000 (99%)\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.000449\n",
      "Train Epoch: 26 [6400/60000 (11%)]\tLoss: 0.074211\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 0.075968\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 0.007795\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.004004\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 0.020846\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.014668\n",
      "Train Epoch: 26 [44800/60000 (75%)]\tLoss: 0.083561\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.026995\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 0.040483\n",
      "\n",
      "Test set: Average loss: 0.0200, Accuracy: 9934/10000 (99%)\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.041985\n",
      "Train Epoch: 27 [6400/60000 (11%)]\tLoss: 0.021766\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 0.115459\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 0.007298\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.055886\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 0.006685\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.019352\n",
      "Train Epoch: 27 [44800/60000 (75%)]\tLoss: 0.031761\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.003708\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 0.008721\n",
      "\n",
      "Test set: Average loss: 0.0191, Accuracy: 9938/10000 (99%)\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.018488\n",
      "Train Epoch: 28 [6400/60000 (11%)]\tLoss: 0.035670\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 0.035204\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 0.024324\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.193959\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 0.033270\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.077301\n",
      "Train Epoch: 28 [44800/60000 (75%)]\tLoss: 0.014207\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.092930\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 0.090991\n",
      "\n",
      "Test set: Average loss: 0.0182, Accuracy: 9941/10000 (99%)\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.002325\n",
      "Train Epoch: 29 [6400/60000 (11%)]\tLoss: 0.043428\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 0.037421\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 0.067155\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.004964\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 0.024056\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.013468\n",
      "Train Epoch: 29 [44800/60000 (75%)]\tLoss: 0.004898\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.050716\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 0.027037\n",
      "\n",
      "Test set: Average loss: 0.0175, Accuracy: 9949/10000 (99%)\n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.018718\n",
      "Train Epoch: 30 [6400/60000 (11%)]\tLoss: 0.103785\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.033070\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 0.077341\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.049357\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tLoss: 0.067477\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.000828\n",
      "Train Epoch: 30 [44800/60000 (75%)]\tLoss: 0.029208\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.082927\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 0.033119\n",
      "\n",
      "Test set: Average loss: 0.0191, Accuracy: 9940/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n > Wider + Deeper teacher training... \")\n",
    "model_ = Net()\n",
    "\n",
    "del model\n",
    "model = model_\n",
    "model.define_wider_deeper()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "for epoch in range(1, 3*(epochs) + 1):\n",
    "    train(epoch)\n",
    "    wider_deeper_teacher_accu = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Teacher:\t98.44999694824219\n",
      " -> Wider model:\t98.4800033569336\n",
      " -> Deeper-Wider model:\t99.12999725341797\n",
      " -> Wider teacher:\t99.02999877929688\n",
      " -> Deeper-Wider teacher:\t99.4000015258789\n"
     ]
    }
   ],
   "source": [
    "print(\" -> Teacher:\\t{}\".format(teacher_accu))\n",
    "print(\" -> Wider model:\\t{}\".format(wider_accu))\n",
    "print(\" -> Deeper-Wider model:\\t{}\".format(deeper_accu))\n",
    "print(\" -> Wider teacher:\\t{}\".format(wider_teacher_accu))\n",
    "print(\" -> Deeper-Wider teacher:\\t{}\".format(wider_deeper_teacher_accu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f94c6b32fbda5dcd64daf382f382d2d5da78e483f351d87e126340144fcf47d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
